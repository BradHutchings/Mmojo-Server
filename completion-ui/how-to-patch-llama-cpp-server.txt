In file ~/llama.cpp/examples/server/server.cpp:

nano ~/llama.cpp/examples/server/server.cpp

Search for: SERVER_STATE_LOADING_MODEL

Replace this blocK:

    auto middleware_server_state = [&res_error, &state](const httplib::Request &, httplib::Response & res) {
        server_state current_state = state.load();
        if (current_state == SERVER_STATE_LOADING_MODEL) {
            res_error(res, format_error_response("Loading model", ERROR_TYPE_UNAVAILABLE));
            return false;
        }
        return true;
    };

With this block:

    auto middleware_server_state = [&res_error, &state](const httplib::Request & req, httplib::Response & res) {
        server_state current_state = state.load();
        if (current_state == SERVER_STATE_LOADING_MODEL) {
            httplib::Request & modified_req = (httplib::Request &) req;
            const char* path_c = modified_req.path.c_str();
            int path_c_len = strlen(path_c);
            char last_five[6];
            strcpy(last_five, path_c + (path_c_len - 5));

            if ((strcmp(path_c, "/") == 0) || (strcmp(last_five, ".html") == 0)) {
                modified_req.path = "/loading.html";
            }
            else {
                res_error(res, format_error_response("Loading model", ERROR_TYPE_UNAVAILABLE));
                return false;
            }
        }
        return true;
    };

--------------------------------------------------------------------------------

Search for: model_meta()

Replace this block:

    json model_meta() const {
        return json {
            {"vocab_type",  llama_vocab_type    (model)},
            {"n_vocab",     llama_n_vocab       (model)},
            {"n_ctx_train", llama_n_ctx_train   (model)},
            {"n_embd",      llama_n_embd        (model)},
            {"n_params",    llama_model_n_params(model)},
            {"size",        llama_model_size    (model)},
        };
    }

With this block:

    json model_meta() const {
        char general_architecture[64];
        char general_type[64];
        char general_name[64];
        char general_version[64];
        char general_finetune[64];
        char general_basename[64];
        char general_size_label[64];
        char general_license[64];

        general_architecture[0] = 0;
        general_type[0] = 0;
        general_name[0] = 0;
        general_version[0] = 0;
        general_finetune[0] = 0;
        general_basename[0] = 0;
        general_size_label[0] = 0;
        general_license[0] = 0;

        llama_model_meta_val_str(model, "general.architecture", general_architecture, 64);
        llama_model_meta_val_str(model, "general.type", general_type, 64);
        llama_model_meta_val_str(model, "general.name", general_name, 64);
        llama_model_meta_val_str(model, "general.version",      general_version, 64);
        llama_model_meta_val_str(model, "general.finetune",     general_finetune, 64);
        llama_model_meta_val_str(model, "general.basename",     general_basename, 64);
        llama_model_meta_val_str(model, "general.size_label",   general_size_label, 64);
        llama_model_meta_val_str(model, "general.license",      general_license, 64);

        return json {
            {"vocab_type",  llama_vocab_type            (vocab)},
            {"n_vocab",     llama_vocab_n_tokens        (vocab)},
            {"n_ctx_train", llama_n_ctx_train           (model)},
            {"n_embd",      llama_n_embd                (model)},
            {"n_params",    llama_model_n_params        (model)},
            {"size",        llama_model_size            (model)},
            {"general.architecture", general_architecture },
            {"general.type", general_type },
            {"general.name", general_name },
            {"general.version", general_version },
            {"general.finetune", general_finetune },
            {"general.basename", general_basename },
            {"general.size_label", general_size_label },
            {"general.license", general_license },
        };
    }


    // previous
    json model_meta() const {
		char general_architecture[64];
		char general_type[64];
		char general_name[64];
		char general_version[64];
		char general_finetune[64];
		char general_basename[64];
		char general_size_label[64];
		char general_license[64];
		
		llama_model_meta_val_str(model, "general.architecture",	general_architecture, 64);
		llama_model_meta_val_str(model, "general.type",	general_type, 64);
		llama_model_meta_val_str(model, "general.name",	general_name, 64);
		llama_model_meta_val_str(model, "general.version",	general_version, 64);
		llama_model_meta_val_str(model, "general.finetune",	general_finetune, 64);
		llama_model_meta_val_str(model, "general.basename",	general_basename, 64);
		llama_model_meta_val_str(model, "general.size_label",	general_size_label, 64);
		llama_model_meta_val_str(model, "general.license",	general_license, 64);

        return json {
            {"vocab_type",  llama_vocab_type    (model)},
            {"n_vocab",     llama_n_vocab       (model)},
            {"n_ctx_train", llama_n_ctx_train   (model)},
            {"n_embd",      llama_n_embd        (model)},
            {"n_params",    llama_model_n_params(model)},
            {"size",        llama_model_size    (model)},
			{"general.architecture", general_architecture },
			{"general.type", general_type },
			{"general.name", general_name },
			{"general.version", general_version },
			{"general.finetune", general_finetune },
			{"general.basename", general_basename },
			{"general.size_label", general_size_label },
			{"general.license", general_license },
        };
    }


--------------------------------------------------------------------------------

For llamafile, server.cpp

nano ~/llamafile/llama.cpp/server/server.cpp

Search for: json models =

Replace this block:

    svr.Get("/v1/models", [&params](const httplib::Request& req, httplib::Response& res)
            {
                res.set_header("Access-Control-Allow-Origin", req.get_header_value("Origin"));
                std::time_t t = std::time(0);

                json models = {
                    {"object", "list"},
                    {"data", {
                        {
                            {"id", params.model_alias},
                            {"object", "model"},
                            {"created", t},
                            {"owned_by", "llamacpp"}
                        },
                    }}
                };

                res.set_content(models.dump(), "application/json; charset=utf-8");
            });

With this block:

    svr.Get("/v1/models", [&llama, &params](const httplib::Request& req, httplib::Response& res)
            {
                res.set_header("Access-Control-Allow-Origin", req.get_header_value("Origin"));
                std::time_t t = std::time(0);

                char general_architecture[64];
                char general_type[64];
                char general_name[64];
                char general_version[64];
                char general_finetune[64];
                char general_basename[64];
                char general_size_label[64];
                char general_license[64];
                
                llama_model_meta_val_str(llama.model, "general.architecture",	general_architecture, 64);
                llama_model_meta_val_str(llama.model, "general.type",	general_type, 64);
                llama_model_meta_val_str(llama.model, "general.name",	general_name, 64);
                llama_model_meta_val_str(llama.model, "general.version",	general_version, 64);
                llama_model_meta_val_str(llama.model, "general.finetune",	general_finetune, 64);
                llama_model_meta_val_str(llama.model, "general.basename",	general_basename, 64);
                llama_model_meta_val_str(llama.model, "general.size_label",	general_size_label, 64);
                llama_model_meta_val_str(llama.model, "general.license",	general_license, 64);

                json models = {
                    {"object", "list"},
                    {"data", {
                        {
                            {"id", params.model_alias},
                            {"object", "model"},
                            {"created", t},
                            {"owned_by", "llamacpp"},
                            {"meta", {
                                {"vocab_type",  llama_vocab_type    (llama.model)},
                                {"n_vocab",     llama_n_vocab       (llama.model)},
                                {"n_ctx_train", llama_n_ctx_train   (llama.model)},
                                {"n_embd",      llama_n_embd        (llama.model)},
                                {"n_params",    llama_model_n_params(llama.model)},
                                {"size",        llama_model_size    (llama.model)},
                                {"general.architecture", general_architecture},
                                {"general.type", general_type},
                                {"general.name", general_name},
                                {"general.version", general_version},
                                {"general.finetune", general_finetune},
                                {"general.basename", general_basename},
                                {"general.size_label", general_size_label},
                                {"general.license", general_license},
                            }},
                        },
                    }}
                };

                res.set_content(models.dump(), "application/json; charset=utf-8");
            });

--------------------------------------------------------------------------------



https://github.com/ggerganov/llama.cpp/blob/3ba780e2a8f0ffe13f571b27f0bbf2ca5a199efc/examples/server/server.cpp#L2673

--------------------------------------------------------------------------------

sed -i ':a;N;$!ba;s/    auto middleware_server_state = [&res_error, &state](const httplib::Request &, httplib::Response & res) {
        server_state current_state = state.load();
        if (current_state == SERVER_STATE_LOADING_MODEL) {
            res_error(res, format_error_response("Loading model", ERROR_TYPE_UNAVAILABLE));
            return false;
        }
        return true;
    };
/FART/g' llama.cpp/example/server/server.cpp



ORIG="    auto middleware_server_state = [&res_error, &state](const httplib::Request &, httplib::Response & res) {
        server_state current_state = state.load();
        if (current_state == SERVER_STATE_LOADING_MODEL) {
            res_error(res, format_error_response(\"Loading model\", ERROR_TYPE_UNAVAILABLE));
            return false;
        }
        return true;
    };
"

REPLACEMENT="FART"

sed -i ":a;N;$!ba;s/${ORIG}/${REPLACEMENT}/g" llama.cpp/examples/server/server.cpp

cp llama.cpp/examples/server/server.cpp server-save.cpp
cp server-save.cpp llama.cpp/examples/server/server.cpp




gawk '
    auto middleware_server_state = [&res_error, &state](const httplib::Request &, httplib::Response & res) {
        server_state current_state = state.load();
        if (current_state == SERVER_STATE_LOADING_MODEL) {
            res_error(res, format_error_response("Loading model", ERROR_TYPE_UNAVAILABLE));
            return false;
        }
        return true;
    };
' server-save.cpp